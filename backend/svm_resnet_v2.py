# -*- coding: utf-8 -*-
"""svm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pXxn0dq-Q2VENz_BuLNZX-U6COHTrZgQ
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve,auc,classification_report,confusion_matrix,accuracy_score
from sklearn.svm import SVC
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.inception_resnet_v2 import InceptionResNetV2
from keras.applications.inception_resnet_v2 import preprocess_input
from keras.preprocessing import image
from keras.layers import Flatten
from keras import Sequential
import numpy as np
import os
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from keras.models import Model
from pickle import dump
from imutils import paths
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import glob
import argparse
import imutils
import pickle
import time
import cv2
import os
from numpy import asarray
from numpy import save
from numpy import load	

model = InceptionResNetV2(weights='imagenet', include_top=False)
new_model = Sequential()
new_model.add(model)
new_model.add(Flatten())
new_model.summary()

def train(trainX, trainY):
	start = time.time()
	# define support vector classifier
	svm = SVC(kernel='linear')
	train_history = svm.fit(trainX, trainY)
	end = time.time()
	print("Training time: %s seconds" % str(end - start))
	pickle.dump(svm, open(model_name, 'wb'))
	print("%s saved\n" %model_name)	

def test(testX, testY):
	loaded_svm = pickle.load(open(model_name, 'rb'))
	start = time.time()
	y_pred = loaded_svm.predict(testX)
	end = time.time()
	print("Predict time: %s seconds" % str(end - start))
	print("Confusion_matrix", confusion_matrix(testY,y_pred))
	target_names=['NORMAL','COVID-19','PNEUMONIA']
	print(classification_report(testY,y_pred,target_names=target_names))
	print("Accuracy : ", accuracy_score(testY, y_pred))


# construct the argument parse and parse the arguments
# ap = argparse.ArgumentParser()
# ap.add_argument("-tr", "--train", required=True,
# 	help="path to input trainset ")
# ap.add_argument("-t", "--test", required=True,
# 	help="path to input testset ")
# args = vars(ap.parse_args())

# grab the list of images that we'll be describing
# imageTrainPaths = list(paths.list_images(args["train"]))
# imageTestPaths = list(paths.list_images(args["test"]))

imageTrainPaths = list(paths.list_images("dataset/train"))
imageTestPaths = list(paths.list_images("dataset/test"))

def get_data(imagePaths, t):
	images = []
	labels = []
	labelDict = {"NORMAL":0, "COVID-19":1, "PNEUMONIA":2}
	# loop over the input images
	for (i, imagePath) in enumerate(imagePaths):
		label = imagePath.split(os.path.sep)[-2]
		img = image.load_img(imagePath, target_size=(299, 299))

		# Keras provides the img_to_array() function for converting a loaded image in\
		# PIL format into a NumPy array for use with deep learning models. 
		# The img_to_array() function adds channels: x.shape = (224, 224, 3)\
		# for RGB and (224, 224, 1) for gray image

		img_data = image.img_to_array(img)
		
		# The expand_dims() function is used to expand the shape of an array\
		# Insert a new axis that will appear at the axis position in the expanded array shape.
		# In order to create a batch of images, you need an additional dimension: 
		# (samples, size1,size2,channels)

		img_data = np.expand_dims(img_data,axis=0)

		# The preprocess_input function is meant to adequate your image to the format the model requires.
		# preprocess_input subtracts the mean RGB channels of the imagenet dataset.\
		# This is because the model you are using has been trained on a different dataset: x.shape is still (1, 224, 224, 3)

		img_data = preprocess_input(img_data)
		images.append(img_data)
		labels.append(labelDict[label])
	# 	print("[INFO] processed {}/{}".format(imagePath, str(labelDict[label])))
	# 	print("[INFO] feature: {}".format(feature))
	# #save feature	

	# If you add x to an array images, at the end of the loop, you need to add images = np.vstack(images)\
	# so that you get (n, 224, 224, 3) as the dim of images where n is the number of images processed

	images = np.vstack(images)

	features = new_model.predict(images)
	print("features:", type(features))
	labels = np.array(labels)
	print("labels:", type(labels))
	#features =  np.array(features)
	save(t + 'feature_dense_net.npy', features)
	save(t + 'label_dense_net.npy', labels)
	return features, labels

model_name =  'svm_feature_dense_net.sav'
print("start processing data...............")	

trainX, trainY = get_data(imageTrainPaths, "train")
testX, testY = get_data(imageTestPaths, "test")

sc_x = StandardScaler() 

# fit_transform() is used on the training data so that we can scale the training data and also learn \
# the scaling parameters of that data.
trainX = sc_x.fit_transform(trainX)  

# Using the transform method we can use the same mean and variance as it is calculated from our training \
# data to transform our test data
testX = sc_x.transform(testX)

train(trainX, trainY)

test(testX, testY)